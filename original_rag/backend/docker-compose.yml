version: '3.8'

services:
  # === Backend API ===
  api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    environment:
      - DEBUG=false
      - API_HOST=0.0.0.0
      - API_PORT=8001
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3}
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-ollama}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text}
      - VECTOR_PROVIDER=${VECTOR_PROVIDER:-qdrant}
      - QDRANT_URL=http://qdrant:6333
      - MEMORY_BACKEND=redis
      - REDIS_URL=redis://redis:6379
    depends_on:
      - qdrant
      - redis
      - ollama
    volumes:
      - ./data:/app/data
    networks:
      - rag-network
    restart: unless-stopped

  # === Vector Database ===
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - rag-network
    restart: unless-stopped

  # === Cache / Memory ===
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - rag-network
    restart: unless-stopped

  # === LLM (Ollama) ===
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - rag-network
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # === Frontend ===
  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - VITE_API_URL=http://localhost:8001
    depends_on:
      - api
    networks:
      - rag-network
    restart: unless-stopped

networks:
  rag-network:
    driver: bridge

volumes:
  qdrant_data:
  redis_data:
  ollama_data:
